{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/p6g2f1xs1qq5s4_nn_mvnygc0000gn/T/ipykernel_20684/3441374631.py:55: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['Description'] = data['Description'].str.replace('[^\\w\\s]',' ')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import glob as glob\n",
    "import os\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def main():\n",
    "\n",
    "    path = define_path()\n",
    "    data = concat_files(path)\n",
    "    data = transform_data(data)\n",
    "    data = clean_data(data)\n",
    "    bi_grams = bi_gram_data(data)\n",
    "    skills = skills_data(data)\n",
    "    save_df(data,skills,bi_grams)\n",
    "\n",
    "def define_path():\n",
    "    path= input('Please define path of your xlsx files with  * to add all xlsx you want to concat')\n",
    "    return path\n",
    "\n",
    "def concat_files(path):\n",
    "    files=glob.glob(path)\n",
    "    data=pd.concat(map(pd.read_excel,files),ignore_index=True)\n",
    "    return data\n",
    "    \n",
    "def transform_data(data):\n",
    "    data=data.drop_duplicates(['Name','Company'])\n",
    "    data=data.drop('Column1',axis=1)\n",
    "    sep=data['Contract'].str.split(' ',n=1,expand= True)\n",
    "    data=data.drop('Contract',axis=1)\n",
    "    data['Contract']=sep.iloc[:,0].to_list()\n",
    "    data['Level']=sep.iloc[:,1].to_list()\n",
    "\n",
    "    data['Level']=data['Level'].str.replace('· ','')\n",
    "\n",
    "    sep=data['Industry'].str.split(' ',n=2,expand= True)\n",
    "    data=data.drop('Industry',axis=1)\n",
    "\n",
    "    data['Size ']=sep.iloc[:,0].to_list()\n",
    "    data['Industry']=sep.iloc[:,2].to_list()\n",
    "    data['Industry']=data['Industry'].str.replace('· ','')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    from gensim.parsing.preprocessing import remove_stopwords\n",
    "     ## Lower case\n",
    "\n",
    "    for column in data.columns:\n",
    "        \n",
    "        data[column] = data[column].str.lower()\n",
    "         \n",
    "     ## remove tabulation and punctuation\n",
    "    data['Description'] = data['Description'].str.replace('[^\\w\\s]',' ')\n",
    "     ## digits\n",
    "    #data['Description'] = data['Description'].str.replace('\\d+', '')\n",
    "\n",
    "     # Remove stopwords\n",
    "    data['Description']=data['Description'].apply(lambda x: remove_stopwords(x))\n",
    "    \n",
    "\n",
    "     #Remove Serveral words\n",
    "    data['Description'] = data['Description'].str.replace('costa rica', '')\n",
    "    data['Description'] = data['Description'].str.replace('equal opportunity', '')\n",
    "    data['Description'] = data['Description'].str.replace('sexual orientation', '')\n",
    "    data['Description'] = data['Description'].str.replace('gender identity', '')\n",
    "    data['Description'] = data['Description'].str.replace('team member', '')\n",
    "    data['Description'] = data['Description'].str.replace('national origin', '')\n",
    "    data['Description'] = data['Description'].str.replace('data analytics', 'data analysis')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def bi_gram_data(data, n=2):\n",
    "\n",
    "    bag_words_data = pd.DataFrame(columns=['Bi-gram','Count','Data Postion'])\n",
    "\n",
    "    Bi_gram = []\n",
    "    Count = []\n",
    "    Data_pos = []\n",
    "     \n",
    "    for position in data['Data Position'].unique().tolist():\n",
    "        \n",
    "        text=data[data['Data Position']== position]['Description']\n",
    "    \n",
    "        new= text.str.split()\n",
    "        new=new.values.tolist()\n",
    "        corpus=[word for i in new for word in i]\n",
    "\n",
    "        def _get_top_ngram(corpus, n=None):\n",
    "             vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "             bag_of_words = vec.transform(corpus)\n",
    "             sum_words = bag_of_words.sum(axis=0) \n",
    "             words_freq = [(word, sum_words[0, idx]) \n",
    "                      for word, idx in vec.vocabulary_.items()]\n",
    "             words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "             return words_freq\n",
    "\n",
    "        top_n_bigrams=_get_top_ngram(text,n)\n",
    "         \n",
    "        for i in range(len(top_n_bigrams)):\n",
    "            \n",
    "            Bi_gram.append(top_n_bigrams[i][0])\n",
    "            Count.append(top_n_bigrams[i][1])\n",
    "            Data_pos.append(position)\n",
    "\n",
    "    bag_words_data['Bi-gram'] = Bi_gram\n",
    "          \n",
    "    bag_words_data['Count'] = Count\n",
    "\n",
    "    bag_words_data['Data Postion'] = Data_pos\n",
    "\n",
    "\n",
    "    return bag_words_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def skills_data(data):\n",
    "    \n",
    "    skills_df=pd.DataFrame(columns=['Name','Count','Data Position'])\n",
    "    \n",
    "    name=[]\n",
    "    count_skill=[]\n",
    "    data_pos=[]\n",
    "\n",
    "    \n",
    "    for position in data['Data Position'].unique().tolist():\n",
    "        \n",
    "         text=data[data['Data Position']== position]['Description']\n",
    "    \n",
    "         text= text.tolist()\n",
    "         text=' '.join(text)\n",
    "\n",
    "         skills= ['python','sql','scala','excel',' r ','julia','spark','cloud','azure','aws','databricks','html','knime','git hub','tableau','power bi',' dash ','shiny','google looker','hadoop',' sas ','matlab','version control','keras','pytorch','deep learning']\n",
    "\n",
    "    \n",
    "\n",
    "         for skill in skills:\n",
    "\n",
    "             name.append(skill)\n",
    "\n",
    "             count_skill.append(text.count(skill))\n",
    "\n",
    "             data_pos.append(position)\n",
    "\n",
    "\n",
    "\n",
    "    skills_df['Name'] = name\n",
    "    skills_df['Count'] = count_skill\n",
    "    skills_df['Data Position'] = data_pos\n",
    "\n",
    "    skills_df=skills_df.sort_values('Count',ascending=False)\n",
    "\n",
    "    return skills_df\n",
    "\n",
    "\n",
    "def save_df(data,skills,bi_grams):\n",
    "      # save the df\n",
    "           skills.to_excel('skills_count.xlsx', engine='xlsxwriter')\n",
    "           bi_grams.to_excel('bi_grams_count.xlsx',engine='xlsxwriter')\n",
    "           data.to_excel('linkedin data jobs.xlsx',engine='xlsxwriter')\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
